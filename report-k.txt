My solution to the assignment is based on creating an optimal state description, which in turn simplifies the getReward function. Based on testing we found out that to maximize the travel distance we need to pick up as much fuel as possible, so that is the primary drive of the agent. In case there is no fuel in the current state, we choose the longest empty lane and travel it. We also discovered the different fuel consumption with relation to speed, and that is the faster we go, the less fuel is consumed, so we reward higher speeds, if at all possible, but in the case our way to the fuel is blocked, the agent slows down. The chosen approach could be optimized that the agent would overtake slow veichles, if there is enough space between the car and the fuel package, but it wouldn't be completely safe, as the car can speed up at any time and prevent us from collecting the fuel package, so the safest route is to just follow it. The agent travels between 14k and 16k units, however the learning isn't optimal, as it learns well only in about 80% cases. 